{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__\n",
    "\n",
    "### 本节中习惯不好，以后张量最好用大写字体表示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "11.3\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from torch.backends import  cudnn\n",
    "import torch # 如果pytorch安装成功即可导入\n",
    "print(torch.cuda.is_available()) # 查看CUDA是否可用\n",
    "print(torch.cuda.device_count()) # 查看可用的CUDA数量\n",
    "print(torch.version.cuda) # 查看CUDA的版本号\n",
    "print(cudnn.is_available())  #返回True则说明已经安装了cuDNN\n",
    "\n",
    "# ngpu= 1\n",
    "# # Decide which device we want to run on\n",
    "# device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "# print(device)\n",
    "# print(torch.cuda.get_device_name(0))\n",
    "# print(torch.rand(3,3).cuda())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 4, 2],\n",
      "        [5, 6, 2]])\n",
      "torch.Size([2, 3])\n",
      "<built-in method size of Tensor object at 0x000002AD8CE9A130>\n",
      "2\n",
      "3\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### 维度基本信息获取\n",
    "\n",
    "x = torch.tensor([[3,4,2],[5,6,2]])\n",
    "print(x)\n",
    "print(x.shape)\n",
    "\n",
    "print(x.size)\n",
    "print(x.size(0))\n",
    "print(x.size(1))\n",
    "print(x.numel())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[-0.2791, -1.0378, -1.1596, -0.8834],\n",
      "        [ 1.8179,  0.4698,  0.1582,  1.0156],\n",
      "        [-0.3633,  1.4958,  0.4013, -0.4824]])\n",
      "tensor([[5.4174e-01, 9.9210e-01, 6.8271e-04, 7.5525e-01],\n",
      "        [4.4803e-01, 4.0747e-01, 6.0206e-01, 8.2881e-01],\n",
      "        [3.9961e-01, 4.9765e-01, 1.9458e-01, 9.6983e-01]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### 基本矩阵形式生成\n",
    "\n",
    "y = torch.arange(12)\n",
    "print(y)\n",
    "y = y.reshape(3,4)\n",
    "print(y)\n",
    "\n",
    "print(torch.zeros((3,4)))\n",
    "print(torch.ones((3,4)))\n",
    "\n",
    "# print(torch.ones(3,4)) 最好还是按照上面写\n",
    "\n",
    "print(torch.randn((3,4))) #均值0，方差1的正态分布\n",
    "\n",
    "\n",
    "print(torch.rand((3,4))) # 0-1 的随机数\n",
    "\n",
    "\n",
    "'''\n",
    "torch.Tensor()\n",
    "首先明确这一点,这是python类,是默认张量类型torch.FloatTensor()的别名,\n",
    "每次调用torch.Tensor([1,2, 3, 4, 5])来构造一个tensor的时候,会调用Tensor类的构造函数,生成一个单精度浮点类型的张量。除此外还有别的构造方法但是很少用\n",
    "\n",
    "但是torch.tensor(注意这里是小写)仅仅是python的函数,函数原型是\n",
    "\n",
    "torch.tensor(data, dtype=None, device=None, requires_grad=False)\n",
    " 其中data可以是:list,tuple,NumPy,ndarray等其他类型,torch.tensor会从data中的数据部分做拷贝(而不是直接引用),根据原始数据类型生成相应的torch.LongTensor torch.FloatTensor和torch.DoubleTensor\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 3,  4,  6, 10]), tensor([-1,  0,  2,  6]), tensor([ 2,  4,  8, 16]), tensor([0.5000, 1.0000, 2.0000, 4.0000]), tensor([ 1,  4, 16, 64]))\n",
      "tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])\n"
     ]
    }
   ],
   "source": [
    "### \"按元素\"运算\n",
    "\n",
    "x = torch.tensor([1,2,4,8])\n",
    "y = torch.tensor([2,2,2,2])\n",
    "\n",
    "result = x+y,x-y,x*y,x/y,x**y\n",
    "\n",
    "print(result)\n",
    "\n",
    "\n",
    "print(torch.exp(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [ 2.,  1.,  4.,  3.],\n",
      "        [ 1.,  2.,  3.,  4.],\n",
      "        [ 4.,  3.,  2.,  1.]])\n",
      "tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
      "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])\n",
      "tensor([[False,  True, False,  True],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(66.)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 张量连接\n",
    "\n",
    "x = torch.arange(12,dtype = torch.float32).reshape(3,4)\n",
    "y= torch.tensor([[2.0,1,4,3],[1,2,3,4],[4,3,2,1]])\n",
    "\n",
    "\n",
    "print(torch.cat((x,y),dim=0)) #dim=0，轴0，行\n",
    "#可以看作从外层开始数第几个括号，连接的张量不可以有缺\n",
    "print(torch.cat((x,y),dim=1)) #dim=0，轴0，列\n",
    "\n",
    "\n",
    "\n",
    "print(x==y)  ##逻辑判断矩阵\n",
    "\n",
    "x.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2]]) tensor([[0, 1]])\n",
      "tensor([[0, 1],\n",
      "        [1, 2],\n",
      "        [2, 3]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#广播机制\n",
    "\n",
    "## 形状不同的张量进行运算\n",
    "\n",
    "a = torch.arange(3).reshape(3,1)\n",
    "b = torch.arange(2).reshape(1,2)\n",
    "\n",
    "print(a,b)\n",
    "print(a+b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "tensor([ 8.,  9., 10., 11.])\n",
      "tensor([[ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "tensor([5., 6.])\n",
      "tensor([[0., 1., 2., 3.],\n",
      "        [4., 5., 6., 7.]])\n"
     ]
    }
   ],
   "source": [
    "### 索引和切片\n",
    "\n",
    "x = torch.arange(12,dtype = torch.float32).reshape(3,4)\n",
    "\n",
    "print(x)\n",
    "print(x[-1]) #最后一个元素（dim=0）\n",
    "print(x[1:3])   # 1到2行（第1到2个元素 dim=0）\n",
    "print(x[1,1:3])   # 第二行的第1到2个元素 dim=1\n",
    "\n",
    "print(x[0:2, :])   # 第1到2行的所有元素，:表示所有\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2944414293264\n",
      "2944415265312\n",
      "2944415265312\n",
      "2944415265312\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(12,dtype = torch.float32).reshape(3,4)\n",
    "y= torch.tensor([[2.0,1,4,3],[1,2,3,4],[4,3,2,1]])\n",
    "\n",
    "\n",
    "print(id(y) )\n",
    "\n",
    "y = y + x #会重新给y分配内存地址\n",
    "print(id(y) )\n",
    "\n",
    "\n",
    "# 以下则不会\n",
    "y += x\n",
    "print(id(y) )\n",
    "\n",
    "y[:] = y + x\n",
    "print(id(y) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.  3.]\n",
      " [ 4.  5.  6.  7.]\n",
      " [ 8.  9. 10. 11.]]\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "3.200000047683716\n",
      "3.200000047683716\n",
      "3\n",
      "tensor([[ 0.,  4.,  8.],\n",
      "        [ 1.,  5.,  9.],\n",
      "        [ 2.,  6., 10.],\n",
      "        [ 3.,  7., 11.]])\n"
     ]
    }
   ],
   "source": [
    "### 与numpy对象相互转换\n",
    "x = torch.arange(12,dtype = torch.float32).reshape(3,4)\n",
    "y= torch.tensor([[2.0,1,4,3],[1,2,3,4],[4,3,2,1]])\n",
    "\n",
    "A = x.numpy()\n",
    "print(A)\n",
    "\n",
    "B = torch.tensor(A)\n",
    "print(B)\n",
    "\n",
    "\n",
    "A = torch.tensor(3.2)\n",
    "\n",
    "print(A.item()) # .item()  can only convert an array of size 1 to a Python scalar\n",
    "\n",
    "print(float(A))\n",
    "print(int(A))\n",
    "\n",
    "print(B.T) #矩阵转置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AbsTransform', 'AffineTransform', 'Bernoulli', 'Beta', 'Binomial', 'CatTransform', 'Categorical', 'Cauchy', 'Chi2', 'ComposeTransform', 'ContinuousBernoulli', 'CorrCholeskyTransform', 'CumulativeDistributionTransform', 'Dirichlet', 'Distribution', 'ExpTransform', 'Exponential', 'ExponentialFamily', 'FisherSnedecor', 'Gamma', 'Geometric', 'Gumbel', 'HalfCauchy', 'HalfNormal', 'Independent', 'IndependentTransform', 'Kumaraswamy', 'LKJCholesky', 'Laplace', 'LogNormal', 'LogisticNormal', 'LowRankMultivariateNormal', 'LowerCholeskyTransform', 'MixtureSameFamily', 'Multinomial', 'MultivariateNormal', 'NegativeBinomial', 'Normal', 'OneHotCategorical', 'OneHotCategoricalStraightThrough', 'Pareto', 'Poisson', 'PowerTransform', 'RelaxedBernoulli', 'RelaxedOneHotCategorical', 'ReshapeTransform', 'SigmoidTransform', 'SoftmaxTransform', 'SoftplusTransform', 'StackTransform', 'StickBreakingTransform', 'StudentT', 'TanhTransform', 'Transform', 'TransformedDistribution', 'Uniform', 'VonMises', 'Weibull', 'Wishart', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'bernoulli', 'beta', 'biject_to', 'binomial', 'categorical', 'cauchy', 'chi2', 'constraint_registry', 'constraints', 'continuous_bernoulli', 'dirichlet', 'distribution', 'exp_family', 'exponential', 'fishersnedecor', 'gamma', 'geometric', 'gumbel', 'half_cauchy', 'half_normal', 'identity_transform', 'independent', 'kl', 'kl_divergence', 'kumaraswamy', 'laplace', 'lkj_cholesky', 'log_normal', 'logistic_normal', 'lowrank_multivariate_normal', 'mixture_same_family', 'multinomial', 'multivariate_normal', 'negative_binomial', 'normal', 'one_hot_categorical', 'pareto', 'poisson', 'register_kl', 'relaxed_bernoulli', 'relaxed_categorical', 'studentT', 'transform_to', 'transformed_distribution', 'transforms', 'uniform', 'utils', 'von_mises', 'weibull', 'wishart']\n"
     ]
    }
   ],
   "source": [
    " print(dir(torch.distributions)) # 查看模块中所有函数和类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function cumsum in module torch:\n",
      "\n",
      "cumsum(...)\n",
      "    cumsum(input, dim, *, dtype=None, out=None) -> Tensor\n",
      "    \n",
      "    Returns the cumulative sum of elements of :attr:`input` in the dimension\n",
      "    :attr:`dim`.\n",
      "    \n",
      "    For example, if :attr:`input` is a vector of size N, the result will also be\n",
      "    a vector of size N, with elements.\n",
      "    \n",
      "    .. math::\n",
      "        y_i = x_1 + x_2 + x_3 + \\dots + x_i\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor.\n",
      "        dim  (int): the dimension to do the operation over\n",
      "    \n",
      "    Keyword args:\n",
      "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "            If specified, the input tensor is casted to :attr:`dtype` before the operation\n",
      "            is performed. This is useful for preventing data type overflows. Default: None.\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> a = torch.randn(10)\n",
      "        >>> a\n",
      "        tensor([-0.8286, -0.4890,  0.5155,  0.8443,  0.1865, -0.1752, -2.0595,\n",
      "                 0.1850, -1.1571, -0.4243])\n",
      "        >>> torch.cumsum(a, dim=0)\n",
      "        tensor([-0.8286, -1.3175, -0.8020,  0.0423,  0.2289,  0.0537, -2.0058,\n",
      "                -1.8209, -2.9780, -3.4022])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.cumsum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
